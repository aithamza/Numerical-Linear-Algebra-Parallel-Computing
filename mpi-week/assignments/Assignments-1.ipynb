{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c97b7b",
   "metadata": {},
   "source": [
    "# MPI Assignments\n",
    "\n",
    "### Exercise 1: Hello World\n",
    "1. Write an MPI program which prints the message \"Hello World\"\n",
    "2. Modify your program so that each process prints out both its rank and the total number of processes P that the code is running on, i.e. the size of `MPI_COMM_WORLD`.\n",
    "3. Modify your program so that only a single controller process (e.g. rank 0) prints out a message (very useful when you run with hundreds of processes).\n",
    "4. What happens if you omit the final MPI procedure call in your program?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "747cbee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am the proccess 0 among 1\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "#Communicator, Rank and size\n",
    "COMM = MPI.COMM_WORLD\n",
    "SIZE = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "print(\"I am the proccess {RANK} among {SIZE}\".format(RANK =RANK, SIZE =SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcac2620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World from process 0 of 1\n",
      "****************************** Q4 ******************************\n",
      "I am the process 0 of 1\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "#Initialize MPI environment\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "#Get the total number of processes\n",
    "world_size = comm.Get_size()\n",
    "\n",
    "#Get the rank of the current process\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "#print \"Hello World \" message from each process\n",
    "print(f\"Hello World from process {rank} of {world_size}\")\n",
    "\n",
    "\n",
    "\n",
    "### Q3\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"***\"*10,\"Q4\",\"***\"*10)\n",
    "    print(f\"I am the process {rank} of {world_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64183f51",
   "metadata": {},
   "source": [
    "If you omit the final MPI procedure call MPI_Finalize() in your MPI program, the MPI environment will not be properly finalized before the program exits. This may lead to undefined behavior or even cause the program to hang indefinitely.\n",
    "\n",
    "The MPI_Finalize() procedure is responsible for cleaning up any MPI-related resources that were initialized during MPI_Init(). This includes freeing up memory, closing communication channels, and releasing other resources that may have been allocated by the MPI implementation. If you don't call MPI_Finalize(), the MPI environment may not be able to properly release these resources, leading to potential memory leaks or other issues.\n",
    "\n",
    "It is always good practice to include the MPI_Finalize() call at the end of your MPI program to ensure that the MPI environment is properly finalized and all resources are cleaned up before the program exits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382bb412",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 2: Sharing Data\n",
    "Create a program that obtains an integer input from the terminal and distributes it to all the MPI processes.\n",
    "Each process must display its rank and the received value. \n",
    "Keep reading values until a negative integer is entered.\n",
    "**Output Example**\n",
    "```shell\n",
    "10\n",
    "Process 0 got 10\n",
    "Process 1 got 10\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d17d6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a number: 45\n",
      "I am the process 0\n",
      "Enter a number: 45\n",
      "I am the process 0\n",
      "Enter a number: 67\n",
      "I am the process 0\n",
      "Enter a number: 98\n",
      "I am the process 0\n",
      "Enter a number: 0\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "while True:\n",
    "    if RANK == 0:\n",
    "        sendbuf = int(input(\"Enter a number: \"))\n",
    "    else:\n",
    "        sendbuf = 0\n",
    "    sendbuf = COMM.bcast(sendbuf, root=0)\n",
    "    if sendbuf <= 0:\n",
    "        break\n",
    "    if RANK == 0:\n",
    "        print(\"I am the process 0\")\n",
    "    else:\n",
    "        print(\"I am the process {rank}, I received data {data} from 0\".format(rank=RANK, data=sendbuf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5179b567",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 3 Sending in a ring (broadcast by ring)\n",
    "\n",
    "Write a program that takes data from process zero and sends it to all of the other processes by sending it in a ring. That is, process i should receive the data add the rank of the process to it then send it to process i+1, until the last process is reached.\n",
    "Assume that the data consists of a single integer. Process zero reads the data from the user.\n",
    "print the process rank and the value received.\n",
    "\n",
    "\n",
    "![ring](../data/ring.gif)\n",
    "\n",
    "You may want to use these MPI routines in your solution:\n",
    "`Send` `Recv` \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "size = COMM.Get_size()\n",
    "\n",
    "while True:\n",
    "    if RANK == 0:\n",
    "        x = int(input(\"Put a number: \"))\n",
    "        COMM.send(x, RANK +1)\n",
    "    else:\n",
    "        x = COMM.recv(source = RANK-1)\n",
    "        print(\"processus recepteur est : \",RANK)\n",
    "        if RANK < size -1 :\n",
    "            if x <0 : x-= RANK\n",
    "            COMM.send(x + RANK, RANK +1)\n",
    "    if x < 0 :\n",
    "        break\n",
    "    print(\"rank : \", RANK, \"data \",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48ae3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Array:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Transposed Array:\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr1 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(f'Original Array:\\n{arr1}')\n",
    "\n",
    "arr1_transpose = arr1.transpose()\n",
    "\n",
    "print(f'Transposed Array:\\n{arr1_transpose}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f51dc95",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 4: Scattering Matrix\n",
    "1. Create an n by m matrix A on processor 0.\n",
    "2. Use MPI_Scatterv to send parts of the matrix to the other processors.\n",
    "3. Processor 1 receives A(i,j) for i=0 to (n/2)-1 and j=m/2 to m-1.\n",
    "4. Processor 2 receives A(i,j) for i=n/2 to n-1 and j=0 to (m/2)-1.\n",
    "5. Processor 3 receives A(i,j) for i=n/2 to n-1 and j=m/2 to m-1.\n",
    "**Example:** using n=m=8 for simplicity.\n",
    "\n",
    "\n",
    "![N2utM.png](attachment:N2utM.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "534f38d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix on processor 0:\n",
      "[[ 1.  2.  3.  4.  5.  6.  7.  8.]\n",
      " [ 9. 10. 11. 12. 13. 14. 15. 16.]\n",
      " [17. 18. 19. 20. 21. 22. 23. 24.]\n",
      " [25. 26. 27. 28. 29. 30. 31. 32.]\n",
      " [33. 34. 35. 36. 37. 38. 39. 40.]\n",
      " [41. 42. 43. 44. 45. 46. 47. 48.]\n",
      " [49. 50. 51. 52. 53. 54. 55. 56.]\n",
      " [57. 58. 59. 60. 61. 62. 63. 64.]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1696\\633988385.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0msendcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mdispls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0msendcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0msendcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0msendcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "n = 8\n",
    "m = 8\n",
    "if rank == 0:\n",
    "    n = 8\n",
    "    m = 8\n",
    "    A = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            A[i, j] = i * m + j + 1\n",
    "    print(\"Original matrix on processor 0:\")\n",
    "    print(A)\n",
    "# Divide the matrix into parts to send to each processor\n",
    "    sendcounts = np.zeros(size, dtype=int)\n",
    "    displs = np.zeros(size, dtype=int)\n",
    "    sendcounts[1] = (n // 2) * (m - m // 2)\n",
    "    sendcounts[2] = (n - n // 2) * (m // 2)\n",
    "    sendcounts[3] = (n - n // 2) * (m - m // 2)\n",
    "    displs[1] = (n // 2)\n",
    "    displs[2] = (m //2)\n",
    "    displs[3] = (n - n // 2) * (m - m // 2)\n",
    "else:\n",
    "    A = None\n",
    "    sendcounts = None\n",
    "    displs = None\n",
    "# Scatter the matrix parts to each processor\n",
    "recvA = np.zeros((n // 2, m // 2))\n",
    "recvcounts = (n // 2) * (m // 2)\n",
    "print(np.transpose(A))\n",
    "comm.Scatterv([np.transpose(A) , sendcounts, displs, MPI.DOUBLE], recvA, root=0)\n",
    "if rank == 1:\n",
    "    print(\"Received matrix on processor 1:\")\n",
    "    print(recvA)\n",
    "elif rank == 2:\n",
    "    print(\"Received matrix on processor 2:\")\n",
    "    print(recvA)\n",
    "elif rank == 3:\n",
    "    print(\"Received matrix on processor 2:\")\n",
    "    print(recvA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5135e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = lil_matrix((SIZE, SIZE))\n",
    "A[0, :100] = rand(100)\n",
    "A[1, 100:200] = A[0, :100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7070aa18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d1644",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Exercise 5 Matrix vector product\n",
    "\n",
    "1. Use the `MatrixVectorMult.py` file to implement the MPI version of matrix vector multiplication.\n",
    "2. Process 0 compares the result with the `dot` product.\n",
    "3. Plot the scalability of your implementation. \n",
    "\n",
    "**Output Example**\n",
    "```shell\n",
    "CPU time of parallel multiplication using 2 processes is  174.923446\n",
    "The error comparing to the dot product is : 1.4210854715202004e-14\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f001346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in List of Lists format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "from numpy.random import rand, seed\n",
    "from numba import njit\n",
    "from mpi4py import MPI\n",
    "SIZE = 1000\n",
    "A = lil_matrix((SIZE, SIZE))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed800088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrixVectorMult(A, b, x):\n",
    "    \n",
    "    row, col = A.shape\n",
    "    for i in range(row):\n",
    "        a = A[i]\n",
    "        for j in range(col):\n",
    "            x[i] += A[i,j] * b[j]\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c020d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LocalMatrix = lil_matrix((SIZE, SIZE))\n",
    "# Scatter the matrix A\n",
    "b = rand(SIZE)\n",
    "#####################Compute A*b locally#######################################\n",
    "LocalX = np.zeros(SIZE)\n",
    "\n",
    "start = MPI.Wtime()\n",
    "matrixVectorMult(LocalMatrix, b, LocalX)\n",
    "stop = MPI.Wtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ab89fa",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 6: Pi calculation\n",
    "An approximation to the value π can be obtained from the following expression\n",
    "\n",
    "![Pi expression](../data/pi.PNG)\n",
    "\n",
    "where the answer becomes more accurate with increasing N. Iterations over i are independent so the\n",
    "calculation can be parallelized.\n",
    "\n",
    "For the following exercises you should set N = 840. This number is divisible by 2, 3, 4, 5, 6, 7 and 8\n",
    "which is convenient when you parallelize the calculation!\n",
    "\n",
    "1. Create a program where each process independently computes the value of `π` and prints it to the screen. Check that the values are correct (each process should print the same value)\n",
    "2. Now arrange for different processes to do the computation for different ranges of i. For example, on two processes: rank 0 would do i = 0, 1, 2, . . . , N/2 - 1; rank 1 would do i = N/2, N/2 + 1, . . . , N-1.\n",
    "Print the partial sums to the screen and check the values are correct by adding them up by hand.\n",
    "3. Now we want to accumulate these partial sums by sending them to the controller (e.g. rank 0) to add up:\n",
    "- all processes (except the controller) send their partial sum to the controller\n",
    "- the controller receives the values from all the other processes, adding them to its own partial sum\n",
    "1. Use the function `MPI_Wtime` (see below) to record the time it takes to perform the calculation. For a given value of N, does the time decrease as you increase the number of processes? Note that to ensure that the calculation takes a sensible amount of time (e.g. more than a second) you will probably have to perform the calculation of `π` several thousands of times.\n",
    "2. Ensure your program works correctly if N is not an exact multiple of the number of processes P\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99786e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
